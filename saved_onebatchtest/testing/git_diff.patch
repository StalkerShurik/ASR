diff --git a/requirements.txt b/requirements.txt
index b3e884f..58a7c30 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -14,3 +14,6 @@ hydra-core
 black
 isort
 pre-commit
+
+datasets
+wget
\ No newline at end of file
diff --git a/src/configs/datasets/onebatchtest.yaml b/src/configs/datasets/onebatchtest.yaml
index 211ca48..bd7a149 100644
--- a/src/configs/datasets/onebatchtest.yaml
+++ b/src/configs/datasets/onebatchtest.yaml
@@ -3,7 +3,7 @@ train:
   part: "dev-clean"
   max_audio_length: 20.0
   max_text_length: 200
-  limit: 2
+  limit: 10
   instance_transforms: ${transforms.instance_transforms.train}
 # we filter partitions in one batch test to check the pipeline
 # do not filter test dataset, you want to evaluate on the whole dataset
@@ -12,5 +12,5 @@ val:
   part: "dev-clean"
   max_audio_length: 20.0
   max_text_length: 200
-  limit: 2
+  limit: 10
   instance_transforms: ${transforms.instance_transforms.inference}
diff --git a/src/datasets/collate.py b/src/datasets/collate.py
index 3f8a624..3f14d29 100644
--- a/src/datasets/collate.py
+++ b/src/datasets/collate.py
@@ -1,5 +1,5 @@
 import torch
-
+import torch.nn.functional as F

 def collate_fn(dataset_items: list[dict]):
     """
@@ -14,4 +14,52 @@ def collate_fn(dataset_items: list[dict]):
             of the tensors.
     """

-    pass  # TODO
+    text_encoded = dataset_items[0]['text_encoded']
+    audio = dataset_items[0]['audio']
+    spectrogram = dataset_items[0]['spectrogram']
+
+    print(f"COLLATE!!!!! {text_encoded.shape, audio.shape, spectrogram.shape}")
+
+    """
+        dataset[i] : dict_keys(['audio', 'spectrogram', 'text', 'text_encoded', 'audio_path'])
+    """
+
+    batched_dataset_items = {}
+
+    TEXT_ENCODED_MAX_SIZE = 2**8
+    SPECTROGRAM_MAX_SIZE = 2**10
+    AUDIO_MAX_SIZE = 2**18
+
+    for i in range(len(dataset_items)):
+        for key in dataset_items[i]:
+
+            data_piece = dataset_items[i][key]
+
+            if key == "text_encoded":
+                data_piece = data_piece[:,:TEXT_ENCODED_MAX_SIZE]
+                data_piece = F.pad(data_piece, pad=(0, TEXT_ENCODED_MAX_SIZE - data_piece.shape[1], 0, 0))
+
+            if key == "audio":
+                data_piece = data_piece[:,:AUDIO_MAX_SIZE]
+                data_piece = F.pad(data_piece, pad=(0, AUDIO_MAX_SIZE - data_piece.shape[1], 0, 0))
+
+            if key == "spectrogram":
+                data_piece = data_piece[:,:,:SPECTROGRAM_MAX_SIZE]
+                data_piece = F.pad(data_piece, pad=(0, SPECTROGRAM_MAX_SIZE - data_piece.shape[2], 0, 0, 0, 0))
+
+            # if key in ["text_encoded", "audio", "spectrogram"]:
+            #     print(data_piece.shape)
+
+            if key not in batched_dataset_items.keys():
+                if key in ["text_encoded", "audio", "spectrogram"]:
+                    batched_dataset_items[key] = data_piece
+                else:
+                    batched_dataset_items[key] = []
+                    batched_dataset_items[key].append(data_piece)
+            else:
+                if key in ["text_encoded", "audio", "spectrogram"]:
+                    batched_dataset_items[key] = torch.cat((batched_dataset_items[key], data_piece))
+                else:
+                    batched_dataset_items[key].append(data_piece)
+
+    return batched_dataset_items
diff --git a/train.py b/train.py
index e7e6c08..66a847a 100644
--- a/train.py
+++ b/train.py
@@ -23,7 +23,7 @@ def main(config):
         config (DictConfig): hydra experiment config.
     """
     set_random_seed(config.trainer.seed)
-
+
     project_config = OmegaConf.to_container(config)
     logger = setup_saving_and_logging(config)
     writer = instantiate(config.writer, logger, project_config)
@@ -40,13 +40,19 @@ def main(config):
     # batch_transforms should be put on device
     dataloaders, batch_transforms = get_dataloaders(config, text_encoder, device)

+    print("DATALOADER OK")
+
     # build model architecture, then print to console
     model = instantiate(config.model, n_tokens=len(text_encoder)).to(device)
     logger.info(model)

+    print("MODEL OK")
+
     # get function handles of loss and metrics
     loss_function = instantiate(config.loss_function).to(device)

+    print("LOSS OK")
+
     metrics = {"train": [], "inference": []}
     for metric_type in ["train", "inference"]:
         for metric_config in config.metrics.get(metric_type, []):
@@ -64,6 +70,8 @@ def main(config):
     # epoch_len = None or len(dataloader) for epoch-based training
     epoch_len = config.trainer.get("epoch_len")

+    print("OPTIMIZER OK")
+
     trainer = Trainer(
         model=model,
         criterion=loss_function,
